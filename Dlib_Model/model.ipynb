{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup dlib: Face Detector & Landmark Predictor\n",
    "# -----------------------------\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Define Helper Functions\n",
    "# -----------------------------\n",
    "def compute_frontal_score(landmarks):\n",
    "    \"\"\"\n",
    "    Compute the frontal score of a face using eye and nose landmarks.\n",
    "    A higher score (closer to 1) indicates a more frontal (symmetric) face.\n",
    "    \"\"\"\n",
    "    # Extract left (36-41) and right (42-47) eye landmarks\n",
    "    left_eye_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
    "    right_eye_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
    "    left_eye_center = np.mean(left_eye_points, axis=0).astype(int)\n",
    "    right_eye_center = np.mean(right_eye_points, axis=0).astype(int)\n",
    "    eyes_midpoint = (\n",
    "        (left_eye_center[0] + right_eye_center[0]) // 2,\n",
    "        (left_eye_center[1] + right_eye_center[1]) // 2\n",
    "    )\n",
    "    # Nose tip (landmark 30)\n",
    "    nose_tip = (landmarks.part(30).x, landmarks.part(30).y)\n",
    "\n",
    "    # Calculate distance D: perpendicular distance from nose tip to eye line\n",
    "    line_vector = np.array(right_eye_center) - np.array(left_eye_center)\n",
    "    line_length = np.linalg.norm(line_vector)\n",
    "    if line_length == 0:\n",
    "        return 0\n",
    "    perpendicular_vector = np.array([-line_vector[1], line_vector[0]])\n",
    "    D = abs(np.dot((np.array(nose_tip) - np.array(left_eye_center)),\n",
    "                   perpendicular_vector / line_length))\n",
    "    # Project the nose tip onto the eye line and compute d (distance from eyesâ€™ midpoint)\n",
    "    projection_factor = np.dot(np.array(nose_tip) - np.array(left_eye_center), line_vector) / (line_length ** 2)\n",
    "    nose_projection = np.array(left_eye_center) + projection_factor * line_vector\n",
    "    d = np.linalg.norm(nose_projection - np.array(eyes_midpoint))\n",
    "    # Frontal score: higher value indicates a more frontal face\n",
    "    frontal_score = 1 / (1 + (d / D) ** 2) if D != 0 else 0\n",
    "    return frontal_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_face(image, landmarks):\n",
    "    \"\"\"\n",
    "    Splits the given face image into left and right halves based on landmarks.\n",
    "    Returns the two shifted (centered) halves.\n",
    "    \"\"\"\n",
    "    # Compute eye centers\n",
    "    left_eye_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(36, 42)]\n",
    "    right_eye_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(42, 48)]\n",
    "    left_eye_center = np.mean(left_eye_points, axis=0).astype(int)\n",
    "    right_eye_center = np.mean(right_eye_points, axis=0).astype(int)\n",
    "    eyes_midpoint = (\n",
    "        (left_eye_center[0] + right_eye_center[0]) // 2,\n",
    "        (left_eye_center[1] + right_eye_center[1]) // 2\n",
    "    )\n",
    "    # Nose tip (landmark 30)\n",
    "    nose_tip = (landmarks.part(30).x, landmarks.part(30).y)\n",
    "\n",
    "    # Define the eye connecting line\n",
    "    eye_line = np.array(right_eye_center) - np.array(left_eye_center)\n",
    "    eye_line_length = np.linalg.norm(eye_line)\n",
    "    if eye_line_length == 0:\n",
    "        return None, None\n",
    "\n",
    "    # Project the nose tip onto the eye line\n",
    "    projection_factor = np.dot(np.array(nose_tip) - np.array(left_eye_center), eye_line) / (eye_line_length ** 2)\n",
    "    nose_projection = np.array(left_eye_center) + projection_factor * eye_line\n",
    "\n",
    "    # Determine the splitting point as the midpoint between the eyes_midpoint and the nose projection\n",
    "    split_point = (nose_projection + np.array(eyes_midpoint)) / 2\n",
    "    n = eye_line / eye_line_length  # normalized direction along the eye line\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "    # Create a grid of (x,y) coordinates for all pixels in the image\n",
    "    xx, yy = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    coords = np.stack([xx, yy], axis=-1)  # shape: (height, width, 2)\n",
    "    diff = coords - split_point\n",
    "    # Dot product to decide side: pixels with negative dot belong to the left half\n",
    "    dot = diff[..., 0] * n[0] + diff[..., 1] * n[1]\n",
    "    left_mask = (dot < 0).astype(np.uint8) * 255\n",
    "    right_mask = (dot >= 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Apply masks to obtain left/right images\n",
    "    left_image = cv2.bitwise_and(image, image, mask=left_mask)\n",
    "    right_image = cv2.bitwise_and(image, image, mask=right_mask)\n",
    "\n",
    "    # (Optional) Recentering each half\n",
    "    left_nonzero = cv2.findNonZero(left_mask)\n",
    "    right_nonzero = cv2.findNonZero(right_mask)\n",
    "\n",
    "    if left_nonzero is not None:\n",
    "        left_avg = np.mean(left_nonzero, axis=0).astype(int)[0]\n",
    "        diff_left = width // 2 - left_avg[0]\n",
    "    else:\n",
    "        diff_left = 0\n",
    "\n",
    "    if right_nonzero is not None:\n",
    "        right_avg = np.mean(right_nonzero, axis=0).astype(int)[0]\n",
    "        diff_right = right_avg[0] - width // 2\n",
    "    else:\n",
    "        diff_right = 0\n",
    "\n",
    "    M_left = np.float32([[1, 0, diff_left], [0, 1, 0]])\n",
    "    M_right = np.float32([[1, 0, -diff_right], [0, 1, 0]])\n",
    "\n",
    "    left_image_shifted = cv2.warpAffine(left_image, M_left, (width, height))\n",
    "    right_image_shifted = cv2.warpAffine(right_image, M_right, (width, height))\n",
    "\n",
    "    return left_image_shifted, right_image_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processed images: 17063\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Process the LFW Dataset and Save Processed Images\n",
    "# -----------------------------\n",
    "# Fraction of symmetric images to split (e.g., p = 0.3 means 30% of symmetric images will be split)\n",
    "p = 0.3\n",
    "\n",
    "# Set the dataset directory (each subfolder is a person with that person's images)\n",
    "dataset_dir = \"./dataset\"  # Update this to your LFW dataset folder path\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "# Loop over each person's folder\n",
    "for label in os.listdir(dataset_dir):\n",
    "    person_dir = os.path.join(dataset_dir, label)\n",
    "    if not os.path.isdir(person_dir):\n",
    "        continue\n",
    "\n",
    "    # Get image files (assuming .jpg and .png)\n",
    "    image_files = glob.glob(os.path.join(person_dir, \"*.jpg\")) + glob.glob(os.path.join(person_dir, \"*.png\"))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(image_file)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        # Convert image to grayscale for landmark detection\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "        if len(faces) == 0:\n",
    "            continue  # Skip if no face is detected\n",
    "\n",
    "        face = faces[0]  # Assume the first detected face is the target\n",
    "        landmarks = predictor(gray, face)\n",
    "        frontal_score = compute_frontal_score(landmarks)\n",
    "\n",
    "        # For symmetric faces (frontal score >= 0.5), optionally split them into two halves\n",
    "        if frontal_score >= 0.5:\n",
    "            if random.random() < p:\n",
    "                left_img, right_img = split_face(image, landmarks)\n",
    "                if left_img is not None and right_img is not None:\n",
    "                    all_images.append(left_img)\n",
    "                    all_labels.append(label)\n",
    "                    all_images.append(right_img)\n",
    "                    all_labels.append(label)\n",
    "                else:\n",
    "                    all_images.append(image)\n",
    "                    all_labels.append(label)\n",
    "            else:\n",
    "                all_images.append(image)\n",
    "                all_labels.append(label)\n",
    "        else:\n",
    "            # For asymmetric faces, keep the original image\n",
    "            all_images.append(image)\n",
    "            all_labels.append(label)\n",
    "\n",
    "print(\"Total processed images:\", len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed images have been saved to: ./processed_dataset\n"
     ]
    }
   ],
   "source": [
    "# Save processed images to disk so that you don't need to re-run the preprocessing each time\n",
    "output_dir = \"./processed_dataset\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Create subdirectories for each label\n",
    "unique_labels = set(all_labels)\n",
    "for label in unique_labels:\n",
    "    label_dir = os.path.join(output_dir, label)\n",
    "    if not os.path.exists(label_dir):\n",
    "        os.makedirs(label_dir)\n",
    "\n",
    "# Save each image (using a unique filename)\n",
    "for idx, (img, label) in enumerate(zip(all_images, all_labels)):\n",
    "    filename = f\"{label}_{idx}.jpg\"\n",
    "    filepath = os.path.join(output_dir, label, filename)\n",
    "    cv2.imwrite(filepath, img)\n",
    "\n",
    "print(\"Processed images have been saved to:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in processed dataset: 29779\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Load Processed Images Using PyTorch's ImageFolder\n",
    "# -----------------------------\n",
    "# Define transforms: resize to 224x224, convert to tensor, and normalize as expected by MobileNetV2\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Converts to [0,1] and shape (C, H, W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Use ImageFolder to load the processed dataset (it expects the folder structure: processed_dataset/class_name/images)\n",
    "dataset = datasets.ImageFolder(root=output_dir, transform=data_transforms)\n",
    "print(\"Total images in processed dataset:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Split the Dataset and Create DataLoaders\n",
    "# -----------------------------\n",
    "# For example, use an 80/20 split for training and testing\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ironp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ironp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Loss: 8.2574 Acc: 0.0222\n",
      "Epoch 2/10: Loss: 7.7772 Acc: 0.0243\n",
      "Epoch 3/10: Loss: 7.5643 Acc: 0.0262\n",
      "Epoch 4/10: Loss: 7.3484 Acc: 0.0319\n",
      "Epoch 5/10: Loss: 7.0944 Acc: 0.0377\n",
      "Epoch 6/10: Loss: 6.9320 Acc: 0.0434\n",
      "Epoch 7/10: Loss: 6.5875 Acc: 0.0529\n",
      "Epoch 8/10: Loss: 6.2540 Acc: 0.0629\n",
      "Epoch 9/10: Loss: 5.8814 Acc: 0.0751\n",
      "Epoch 10/10: Loss: 5.5637 Acc: 0.0823\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Build and Train the MobileNetV2 Model Using CUDA\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load a pretrained MobileNetV2 model and replace its classifier head\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "num_features = model.last_channel  # typically 1280\n",
    "num_classes = len(dataset.classes)\n",
    "model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.6848 Acc: 0.0759\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Evaluate the Model on the Test Set\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_corrects += torch.sum(preds == labels.data)\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "test_acc = test_corrects.double() / len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
